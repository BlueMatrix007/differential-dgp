{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from diffgp_svi.likelihoods import Gaussian\n",
    "from diffgp_svi.models import DiffGP\n",
    "from diffgp_svi.layers import SVGP_Layer\n",
    "from diffgp_svi.kernels import RBF\n",
    "from diffgp_svi.datasets import Datasets\n",
    "from diffgp_svi.settings import Settings\n",
    "\n",
    "float_type = Settings().tf_float_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**01. Load dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = Datasets(data_path='data/')\n",
    "data = datasets.all_datasets['kin8nm'].get_data(split=1,prop=0.9)\n",
    "Xtrain, Ytrain, Xtest, Ytest, Y_std = [data[_] for _ in ['X', 'Y', 'Xs', 'Ys','Y_std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**02. Model and training settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter      = 3000 \n",
    "num_minibatch = min(10000,Xtrain.shape[0])\n",
    "num_samples   = 4                # Number of MC samples of SDE solutions\n",
    "num_data      = Xtrain.shape[0]\n",
    "num_dim       = Xtrain.shape[1]  # Regression input dimensions\n",
    "num_output    = 1                # Regression output dimensions\n",
    "num_ind       = 100              # Number of inducing variable in each layer \n",
    "flow_time     = 5.0              # SDE integration time\n",
    "flow_nsteps   = 20               # Number of discretizations in Euler Maruyama solver\n",
    "white         = True             # Whitened represetation for both differential and predictor GPs \n",
    "q_diag        = False            # Diagonal posterior assumption for inducing variables. \n",
    "full_cov      = False            # NOTE: full_cov = True is not implemented  yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 03. Session definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start an interactive session and define separate handles for model training and testing\n",
    "config = tf.ConfigProto(allow_soft_placement = True,log_device_placement=False)\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "sess = tf.InteractiveSession(config = config)\n",
    "\n",
    "X_placeholder = tf.placeholder(dtype = float_type,shape=[None,None])\n",
    "Y_placeholder = tf.placeholder(dtype = float_type,shape=[None,1])\n",
    "\n",
    "train_dataset  = tf.data.Dataset.from_tensor_slices((X_placeholder,Y_placeholder))\n",
    "train_dataset  = train_dataset.shuffle(buffer_size=num_minibatch*5).repeat()\n",
    "train_dataset  = train_dataset.batch(num_minibatch).prefetch(buffer_size = num_minibatch)\n",
    "train_iterator = train_dataset.make_initializable_iterator()\n",
    "\n",
    "test_dataset  = tf.data.Dataset.from_tensor_slices((X_placeholder,Y_placeholder))\n",
    "test_dataset  = test_dataset.batch(min(Xtest.shape[0],num_minibatch)).repeat().prefetch(buffer_size = min(Xtest.shape[0],num_minibatch))\n",
    "test_iterator = test_dataset.make_initializable_iterator()\n",
    "\n",
    "handle = tf.placeholder(tf.string, shape=[])\n",
    "iterator = tf.data.Iterator.from_string_handle(handle, train_dataset.output_types, train_dataset.output_shapes)\n",
    "X,Y = iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 04. Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diff GP kernel initialization\n",
    "# here we use a single RBF kernel with ARD in the differential GP\n",
    "# this implementation uses same kernel across all dimensions of SDE\n",
    "# however, in the paper, we use different kernel across dimensions\n",
    "diff_layer_kernel = RBF(input_dim = num_dim,\n",
    "                        init_lengthscales = 1.0*num_dim, \n",
    "                        init_variance    = 0.01)\n",
    "\n",
    "\n",
    "\n",
    "# pred GP kernel initialization\n",
    "pred_layer_kernel = RBF(input_dim = num_dim,\n",
    "                        init_lengthscales = 1.0*num_dim,\n",
    "                        init_variance    = 1.0)\n",
    "\n",
    "# diff GP layer definition\n",
    "diff_layer = SVGP_Layer(kern    = diff_layer_kernel,\n",
    "                        Um      = np.random.randn(num_ind,num_dim), \n",
    "                        Us_sqrt = np.ones((num_dim,num_ind)) if q_diag else np.stack([np.eye(num_ind)]*num_dim), \n",
    "                        Z       = kmeans2(Xtrain,num_ind,minit='points')[0],\n",
    "                        num_outputs = num_dim,\n",
    "                        white       = white)\n",
    "\n",
    "# pred GP layer definition\n",
    "pred_layer = SVGP_Layer(kern    = pred_layer_kernel,\n",
    "                        Um      = np.random.randn(num_ind,num_output),\n",
    "                        Us_sqrt = np.ones((num_output,num_ind)) if q_diag else np.stack([np.eye(num_ind)]*num_output),\n",
    "                        Z       = kmeans2(Xtrain,num_ind,minit='points')[0],\n",
    "                        num_outputs  = num_output,\n",
    "                        white        = white)\n",
    "\n",
    "# model definition\n",
    "model = DiffGP(likelihood  = Gaussian(), \n",
    "               diff_layer  = diff_layer,\n",
    "               pred_layer  = pred_layer,\n",
    "               num_samples = num_samples,\n",
    "               flow_time   = flow_time,\n",
    "               flow_nsteps = flow_nsteps,\n",
    "               num_data    = num_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 05. Optimization objective and summary statistics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # Compute model lowerbound (averaged over samples)\n",
    "    lowerbound = model._build_likelihood(X,Y)\n",
    "\n",
    "    # Minimize negative lowerbound\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(-1*lowerbound)\n",
    "    \n",
    "    # Compute predictive distributions and then generate training summary (log likelihood and RMSE)\n",
    "    predictive_mean, predictive_var = model.predict_y(X,num_samples=5)\n",
    "    predictive_dist = tf.distributions.Normal(loc=predictive_mean*Y_std, scale=Y_std*predictive_var**0.5)\n",
    "    summ_loglik     = predictive_dist.log_prob(Y*Y_std)\n",
    "    summ_loglik     = tf.reduce_mean(tf.reduce_logsumexp(summ_loglik - tf.log(tf.cast(tf.shape(predictive_mean)[0],dtype=float_type)), axis=0))\n",
    "    summ_rmse       = tf.sqrt(tf.reduce_mean(Y_std**2*((tf.reduce_mean(predictive_mean,axis=0) - Y)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow variable and handle initializations\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "\n",
    "train_handle  = sess.run(train_iterator.string_handle())\n",
    "sess.run(train_iterator.initializer,{X_placeholder:Xtrain,Y_placeholder:Ytrain})\n",
    "\n",
    "test_handle = sess.run(test_iterator.string_handle())\n",
    "sess.run(test_iterator.initializer,{X_placeholder:Xtest,Y_placeholder:Ytest})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 05. Model training **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iter  lowerbound  train_rmse   test_rmse    train_ll     test_ll\n",
      "   50     -1.2733      0.1937      0.1894      0.1036      0.1133\n",
      "  100     -1.1328      0.1752      0.1753      0.2282      0.2280\n",
      "  150     -0.9429      0.1382      0.1399      0.4306      0.4262\n",
      "  200     -0.6774      0.0935      0.0979      0.7204      0.7059\n",
      "  250     -0.4702      0.0795      0.0836      0.9492      0.9301\n",
      "  300     -0.3378      0.0726      0.0764      1.1146      1.0886\n",
      "  350     -0.2768      0.0696      0.0732      1.2060      1.1730\n",
      "  400     -0.2480      0.0674      0.0721      1.2554      1.2044\n",
      "  450     -0.2365      0.0666      0.0699      1.2741      1.2378\n",
      "  500     -0.2281      0.0656      0.0697      1.2901      1.2472\n",
      "  550     -0.2261      0.0651      0.0704      1.2987      1.2382\n",
      "  600     -0.2213      0.0645      0.0683      1.3082      1.2660\n",
      "  650     -0.2155      0.0642      0.0685      1.3130      1.2598\n",
      "  700     -0.2105      0.0639      0.0683      1.3188      1.2661\n",
      "  750     -0.2080      0.0636      0.0681      1.3237      1.2686\n",
      "  800     -0.2017      0.0631      0.0680      1.3312      1.2717\n",
      "  850     -0.2001      0.0627      0.0675      1.3352      1.2789\n",
      "  900     -0.1975      0.0623      0.0679      1.3415      1.2757\n",
      "  950     -0.1900      0.0620      0.0672      1.3459      1.2828\n",
      " 1000     -0.1890      0.0617      0.0673      1.3511      1.2838\n",
      " 1050     -0.1856      0.0616      0.0675      1.3528      1.2807\n",
      " 1100     -0.1874      0.0618      0.0670      1.3508      1.2848\n",
      " 1150     -0.1880      0.0615      0.0675      1.3555      1.2812\n",
      " 1200     -0.1838      0.0611      0.0671      1.3593      1.2890\n",
      " 1250     -0.1832      0.0613      0.0665      1.3598      1.2954\n",
      " 1300     -0.1836      0.0610      0.0663      1.3624      1.2957\n",
      " 1350     -0.1788      0.0611      0.0669      1.3621      1.2883\n",
      " 1400     -0.1769      0.0607      0.0668      1.3660      1.2936\n",
      " 1450     -0.1776      0.0608      0.0669      1.3662      1.2919\n",
      " 1500     -0.1766      0.0608      0.0671      1.3668      1.2886\n",
      " 1550     -0.1738      0.0605      0.0663      1.3712      1.2957\n",
      " 1600     -0.1720      0.0605      0.0676      1.3708      1.2845\n",
      " 1650     -0.1719      0.0605      0.0666      1.3718      1.2950\n",
      " 1700     -0.1730      0.0605      0.0663      1.3713      1.2988\n",
      " 1750     -0.1679      0.0604      0.0668      1.3734      1.2917\n",
      " 1800     -0.1676      0.0601      0.0669      1.3763      1.2947\n",
      " 1850     -0.1725      0.0603      0.0665      1.3735      1.2971\n",
      " 1900     -0.1688      0.0602      0.0667      1.3766      1.2943\n",
      " 1950     -0.1708      0.0602      0.0663      1.3752      1.2972\n",
      " 2000     -0.1702      0.0603      0.0668      1.3736      1.2928\n",
      " 2050     -0.1664      0.0601      0.0666      1.3776      1.2960\n",
      " 2100     -0.1632      0.0600      0.0654      1.3789      1.3109\n",
      " 2150     -0.1652      0.0603      0.0665      1.3753      1.3004\n",
      " 2200     -0.1658      0.0601      0.0659      1.3764      1.3021\n",
      " 2250     -0.1652      0.0598      0.0662      1.3808      1.3015\n",
      " 2300     -0.1672      0.0599      0.0665      1.3802      1.2949\n",
      " 2350     -0.1613      0.0600      0.0659      1.3806      1.3070\n",
      " 2400     -0.1649      0.0600      0.0669      1.3796      1.2903\n",
      " 2450     -0.1599      0.0597      0.0660      1.3826      1.3055\n",
      " 2500     -0.1622      0.0600      0.0652      1.3803      1.3143\n",
      " 2550     -0.1642      0.0599      0.0664      1.3799      1.3016\n",
      " 2600     -0.1643      0.0598      0.0660      1.3820      1.3060\n",
      " 2650     -0.1609      0.0596      0.0662      1.3850      1.3020\n",
      " 2700     -0.1635      0.0599      0.0662      1.3820      1.3022\n",
      " 2750     -0.1591      0.0595      0.0657      1.3871      1.3081\n",
      " 2800     -0.1639      0.0597      0.0659      1.3838      1.3076\n",
      " 2850     -0.1629      0.0596      0.0660      1.3846      1.3006\n",
      " 2900     -0.1600      0.0598      0.0654      1.3828      1.3123\n",
      " 2950     -0.1603      0.0597      0.0657      1.3834      1.3054\n",
      " 3000     -0.1635      0.0597      0.0655      1.3836      1.3121\n"
     ]
    }
   ],
   "source": [
    "print('{:>5s}'.format(\"iter\")        + '{:>12s}'.format(\"lowerbound\")+ \\\n",
    "      '{:>12s}'.format(\"train_rmse\") + '{:>12s}'.format(\"test_rmse\")+\\\n",
    "      '{:>12s}'.format(\"train_ll\")   + '{:>12s}'.format(\"test_ll\"))\n",
    "\n",
    "for i in range(1,num_iter+1):\n",
    "    \n",
    "    try:\n",
    "        sess.run(train_op,feed_dict={handle:train_handle})\n",
    "        \n",
    "        # print every 100 iterations \n",
    "        if i % 50 == 0:\n",
    "            \n",
    "            # lowerbound, train rmse, train log lik\n",
    "            _lb,_trn_rmse,_trn_ll = sess.run([lowerbound,summ_rmse,summ_loglik],\n",
    "                                             {handle:train_handle})\n",
    "            # test rmse, test log lik\n",
    "            _tst_rmse,_tst_ll = sess.run([summ_rmse,summ_loglik],\n",
    "                                         {handle:test_handle})\n",
    "            \n",
    "            print('{:>5d}'.format(i)            + '{:>12.4f}'.format(_lb)+\\\n",
    "                  '{:>12.4f}'.format(_trn_rmse) + '{:>12.4f}'.format(_tst_rmse)+\\\n",
    "                  '{:>12.4f}'.format(_trn_ll)   + '{:>12.4f}'.format(_tst_ll))\n",
    "\n",
    "    except KeyboardInterrupt as e:\n",
    "        print(\"stopping training\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 06. Results on test set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_assess(model, assess_model, X, Y):\n",
    "    n_batches = max(int(X.shape[0]/num_minibatch), 1)\n",
    "    lik, sq_diff = [], []\n",
    "    for X_batch, Y_batch in zip(np.array_split(X, n_batches), np.array_split(Y, n_batches)):\n",
    "        l, sq = assess_model(model, X_batch, Y_batch)\n",
    "        lik.append(l)\n",
    "        sq_diff.append(sq)\n",
    "    lik = np.concatenate(lik, 0)\n",
    "    sq_diff = np.array(np.concatenate(sq_diff, 0), dtype=float)\n",
    "    return np.average(lik), np.average(sq_diff)**0.5\n",
    "\n",
    "S = 50\n",
    "def assess_sampled(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X, S)\n",
    "    m = m.eval({X:X_batch});v = v.eval({X:X_batch});\n",
    "    S_lik = np.sum(norm.logpdf(Y_batch*Y_std, loc=m*Y_std, scale=Y_std*v**0.5), 2)\n",
    "    lik = logsumexp(S_lik, (0),b=1/(float(m.shape[0])))\n",
    "    \n",
    "    mean = np.average(m,0)\n",
    "    sq_diff = Y_std**2*((mean - Y_batch)**2)\n",
    "\n",
    "    return lik, sq_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final loglik: 1.3137\n",
      "final rmse  : 0.0652\n"
     ]
    }
   ],
   "source": [
    "lik, rmse = batch_assess(model, assess_sampled, Xtest, Ytest)\n",
    "print(\"final loglik: \"+str(np.round(lik,4)))\n",
    "print(\"final rmse  : \"+str(np.round(rmse,4)))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
